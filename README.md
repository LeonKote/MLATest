# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по итоговой работе выполнил(а):
- Папушев Роман Олегович
- РИ220947
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Клонирование репозитория.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Клонирование репозитория
Проект содержит подмодуль ml-agents, поэтому обязательно производите вытягивание рекурсивно!

Самый простой способ сделать это с помощью командной строки: ```git clone --recursive https://github.com/LeonKote/MLATest.git```

## Цель работы
Сделать игру на Unity, создать оптимальные условия для обучения ML-агента, применить методы машинного обучения. По завершении проекта подготовить отчёт, рассказать о сопутствующих трудностях, их решении и подходах, которые помогли достигнуть поставленной цели.

## Задание 1
### Разработать игру на Unity

- Посмотрев в интернете примеры проектов, на основе которых реализовывали ML-агентов, я заметил, что в качестве первого самостоятельного проекта лучше взять какую-нибудь простую идею из уже существующих. Множество примеров я находил по игре Flappy Bird, и немного подумав, в качестве идеи решил выбрать игру Dinosaur Game

- Dinosaur Game (также известная как "T-Rex Game" или "Chrome Dino Game") - это мини-игра, которая доступна в веб-браузере Google Chrome, когда отсутствует подключение к Интернету. Игра появляется как страница с изображением динозавра и сообщением о том, что "Отсутствует подключение к Интернету"

- Игра начинается, если пользователь нажимает пробел или стрелку вверх. Динозавр начинает бежать, и игрок должен избегать препятствий, прыгая через них с помощью пробела или стрелки вверх. Цель игры - продержаться как можно дольше, избегая препятствий, таких как кактусы и птеродактили

- Эта мини-игра была добавлена в Google Chrome как развлекательная функция для пользователей в момент временного отсутствия подключения к сети

- Поскольку этот проект пишется ближе к новому году, то для разнообразия я решил сделать соответствующий стиль игры

![screenshot](https://i.imgur.com/lsGxAPn.png)

- Суть игры заключается в том, что игрок в роли Деда Мороза должен успеть доставить подарки на Новый год. Спустившись со своих саней, он бежит вперёд по крышам, преодолевая различные препятствия, такие как спустниковые антены или дымоходы. Игроку необходимо перепрыгивать встречающиеся на пути препятствия с помощью клавиши Пробел, стараясь преодолеть как можно большее расстояние, чтобы Дед Мороз успел всем доставить подарки к новому году, тем временем он бежит всё быстрее и быстрее. Табличка с текущим счётом и наилучшим счётом отражает то, как игрок справляется с обязанностями Деда Мороза.

## Задание 2
### Настроить окружение для обучения ML-агента

- Я поставил себе задачу использовать последние версии пакетов, поэтому первая проблема, с которой я столкнулся после развёртывания виртуальной среды Python - установка последней версии пакета ml-agents (release 21). Дело в том, что при установке из репозитория Python'а пакет тянул в качестве зависимости numpy, версия которого уже была несовместима с текущей версией пакета. Непонятно, почему такое могло произойти, но даже в разделе Issues на официальной странице Github этого пакета разработчик отвечал, что проблема уже решена в ветке develop и после этого закрывал тред как решённый

- По этой причине мне и пришлось использовать версию напрямую из репозитория разработчика. Чтобы не перегружать проект, было принято решение убрать репозиторий в подмодуль. Таким же образом я локально импортировал пакет ml-agents в проект Unity

- Следующей задачей стало реализация логики агента, разработка системы вознаграждений:

- см. [скрипт](https://github.com/LeonKote/MLATest/blob/master/Assets/Scripts/JumperAgent.cs).

```cs

using System.Collections;
using System.Collections.Generic;
using Unity.MLAgents;
using Unity.MLAgents.Actuators;
using Unity.MLAgents.Sensors;
using UnityEngine;

public class JumperAgent : Agent
{
	// Ссылка на PlayerController
	private PlayerController controller;

	// Инициализация
	// Start is called before the first frame update
	void Start()
	{
		controller = GetComponent<PlayerController>();
		controller.OnCollision += OnCollision;
		GameManager.Instance.OnSuccessJump += OnSuccessJump;
	}

	// При начале эпизода тренировки перезапускаем уровень
	public override void OnEpisodeBegin()
	{
		GameManager.Instance.Restart();
	}

	// В качестве наблюдаемых параметров используются только
	// расстояние до ближайшего препятствия и текущая скорость игры
	public override void CollectObservations(VectorSensor sensor)
	{
		sensor.AddObservation(GameManager.Instance.DistToObstacle);
		sensor.AddObservation(GameManager.Instance.Speed);
	}

	// При получении дискретного действия присваиваем соответствующее 
	// значение прыжку
	public override void OnActionReceived(ActionBuffers actions)
	{
		controller.IsJumping = actions.DiscreteActions[0] == 1;
	}

	// Поддержка управления с клавиатуры для тестирования
	public override void Heuristic(in ActionBuffers actionsOut)
	{
		var discreteActions = actionsOut.DiscreteActions;
		discreteActions[0] = (int)Input.GetAxis("Jump");
	}

	// При столкновении с препятствием начисляем штраф
	// и заканчиваем эпизод тренировки
	public void OnCollision()
	{
		AddReward(-1.0f);
		EndEpisode();
	}

	// При успешном прыжке через препятствие начисляем награду
	public void OnSuccessJump()
	{
		AddReward(1.0f);
	}
}

```

- Скрипт прикрепляется непосредственно к персонажу и подбирает с него PlayerController, благодаря чему может использовать функцию прыжка с помощью ```controller.IsJumping```, а также скрипт подписывается на необходимые ивенты, такие как столкновение персонажа с препятствием (от PlayerController) и успешного прыжка (система из GameManager)

- Система проверки успешного прыжка реализована следующим образом:

- см. [скрипт](https://github.com/LeonKote/MLATest/blob/master/Assets/Scripts/GameManager.cs).

```cs

IEnumerator SpawnObstacles()
{
	while (true)
	{
		...
		GameObject obstacle = Instantiate(obstaclePrefabs[Random.Range(0, range)]);
		if (closestObstacle == null)
			closestObstacle = obstacle;
		...
	}
}

void FixedUpdate()
{
	...
	TrainJump();
	...
}

private void TrainJump()
{
	// Если ближайшее препятствие ещё не заспавнилось, то возвращаемся
	if (closestObstacle == null)
		return;

	// Расстояние по оси между ближайшим препятствием и персонажем
	DistToObstacle = closestObstacle.transform.position.x - player.transform.position.x;

	// Если препятствие прошло мимо и игрок находится на земле => игрок перепрыгнул
	// препятствие, вызываем ивент и ищем новое ближайшее препятствие
	// Если ближайшего препятствия нет, то дистанцию приравниваем к 12,
	// это дистанция спавна препятствий - Vector3(12, 0, 0)
	if (DistToObstacle < 0 && player.transform.position.y < -0.01f)
	{
		OnSuccessJump.Invoke();
		closestObstacle = GetClosestObstacle();
		if (closestObstacle == null)
			DistToObstacle = 12;
	}
}

// Поиск ближайшего препятствия путём перебирания всех препятствий перед персонажем
private GameObject GetClosestObstacle()
{
	foreach (var obstacle in obstacles)
		if (obstacle.transform.position.x > player.transform.position.x)
			return obstacle;
	return null;
}

```

![screenshot](https://i.imgur.com/EEzHhG0.png)
![screenshot](https://i.imgur.com/kXQOpZA.png)

- Также я пришёл к выводу, что тренировать агента нужно, когда игровая логика приемущественно находится в FixedUpdate(), в противном случае агент не может стабильно обучаться. Поскольку вся логика была написана с использованием Time.deltaTime, то перенести её в FixedUpdate() не составило проблем

- Для удобства я создал два bat-файла - start и tensor. Первый - запускает обучение агента, второй - открывает tensorboard.

## Задание 3
### Обучить ML-агента

- см. [видео с обучением](https://youtu.be/YI1nhkALtmw).

- Ещё одна трудность, которая уже сопровождала выбранный жанр, заключается в том, что на большой скорости объекты, которые превышают скорость обработки физики (по умолчанию 50 раз в секунду), обречены на просчёты в обработке дискретных столковений, из-за чего персонаж может игнорировать столкновения с объектами. Эта проблема ещё более выражена при ускоренном обучении ML-агента.

- Таким образом, я опытным путём выяснил, что у игр могут быть свои особенности, которые стоит учитывать при тренеровке агента. В данном случае - обработка столкновений при ускоренном обучении.

- см. [видео с практикой](https://youtu.be/vv3rEZfwxps).
- см. [билд](https://drive.google.com/file/d/1Ub-sggF0w7ANuk-IVyme828BwdEXNtvB/view?usp=sharing).

![screenshot](https://i.imgur.com/zgwqOMV.png)

| Название модели из results | Название импортированной модели в Unity | Комментарий | 
| :---: | :---: | ----- |
| Test1 | Jumper1 | Первая натренированная модель, стабильно достигает счёта в 1700 |
| Test2 | - | Несостоявшаяся модель |
| Test3 | - | Несостоявшаяся модель |
| Test4 | Jumper2 | Модель не смогла научится преодолевать препятствия |
| Test5 | Jumper3 | Стабильно достигает счёта в 2300 |
| Test6 | - | Несостоявшаяся модель |
| Test7 | Jumper4 | Стабильно достигает счёта в 2500 |

## Выводы

В процессе работы я научился самостоятельно разрабатывать игру на Unity, создать оптимальные условия для обучения ML-агента и применять методы машинного обучения. Я узнал нюансы создания игры для обучения ML-агента, как настраивать окружение для обучения ML-агента и, соответственно, как правильно его обучать

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
